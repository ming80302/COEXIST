{
 "cells": [
  {
   "cell_type": "raw",
   "id": "946e5923-6269-49fe-8df6-d3d8fb26baa4",
   "metadata": {},
   "source": [
    "Using ERA5 data from glade and assigns weather types (WTs) during the North American Monsoon (NAM) season for the Arizona West, Arizona East, New Mexico North, and New Mexico South region.\n",
    "\n",
    "from Andy's colab\n",
    "https://colab.research.google.com/drive/1O0LdKKvKf6yBO-AABMddkmNtprtm-6k8?usp=sharing#scrollTo=GvwdcdL-1Ydd\n",
    "Centroids and Anomaly DATA\n",
    "https://drive.google.com/drive/folders/1xi-0flWw_uZfTifIh4Yf4gvRxhNleW88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25af1b-5220-4713-8572-b26b6b410410",
   "metadata": {},
   "source": [
    "- Define functions for data processing\n",
    "- User setup section\n",
    "- Load preprocessed data for assigning weather types\n",
    "- Calculate ERA5 normalization data  (NOTE: reverse ERA5. Original ERA5 is from north to South)\n",
    "- Associate data to WTs\n",
    "- Plot the ensemble mean WT frequency during the NAM season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cebba1d-598d-4a91-b0ac-5f1f87f4aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from netCDF4 import Dataset\n",
    "import sys, traceback\n",
    "import dateutil.parser as dparser\n",
    "import string\n",
    "from pdb import set_trace as stop\n",
    "import numpy as np\n",
    "import os\n",
    "import mpl_toolkits\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import pylab as plt\n",
    "import scipy\n",
    "import matplotlib.path as mplPath\n",
    "from matplotlib.patches import Polygon as Polygon2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import scipy.spatial.qhull as qhull\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "from pandas.tseries.offsets import MonthEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf4404-b30b-454a-bef5-243de33416d1",
   "metadata": {},
   "source": [
    "#### Define functions for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f52d368c-1b8e-4123-a394-092cb05b8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation functions\n",
    "def interp_weights(xy, uv,d=2):\n",
    "    tri = qhull.Delaunay(xy)\n",
    "    simplex = tri.find_simplex(uv)\n",
    "    vertices = np.take(tri.simplices, simplex, axis=0)\n",
    "    temp = np.take(tri.transform, simplex, axis=0)\n",
    "    delta = uv - temp[:, d]\n",
    "    bary = np.einsum('njk,nk->nj', temp[:, :d, :], delta)\n",
    "    return vertices, np.hstack((bary, 1 - bary.sum(axis=1, keepdims=True)))\n",
    "\n",
    "def interpolate(values, vtx, wts):\n",
    "    return np.einsum('nj,nj->n', np.take(values, vtx), wts)\n",
    "\n",
    "# ###################################################\n",
    "# ###################################################\n",
    "def PreprocessWTdata(DailyVarsInput,                 # WT data [time,lat,lon,var]\n",
    "                     RelAnnom=1,                     # calculate relative anomalies [1-yes; 0-no]\n",
    "                     SmoothSigma=0,                  # Smoothing stddev (Gaussian smoothing)\n",
    "                     RemoveAnnualCycl=1,             # remove annual cycle [1-yes; 0-no]\n",
    "                     NormalizeData='D',              # normalize variables | options are  - 'C' - climatology\n",
    "                                                     # - 'D' - daily (default)\n",
    "                                                     # - 'N' - none\n",
    "                     ReferencePer=None,              # period for normalizing the data\n",
    "                     Normalize = None):              # mean, std, and spatial mean for normalization\n",
    "    \n",
    "\n",
    "    DailyVars = np.copy(DailyVarsInput)\n",
    "    # Calculate relative anomaly\n",
    "    if RelAnnom == 1:\n",
    "        # we have to work with absolute values for this since we risk to divide by zero values in the climatology\n",
    "        DailyVars=np.abs(DailyVars)\n",
    "        if Normalize is None:\n",
    "            if ReferencePer is None:\n",
    "                DailyVars=(DailyVars-np.mean(DailyVars, axis=0)[None,:])/np.mean(DailyVars, axis=0)[None,:]\n",
    "            else:\n",
    "                DailyVars=(DailyVars-np.mean(DailyVars[ReferencePer], axis=0)[None,:])/np.mean(DailyVars[ReferencePer], axis=0)[None,:]\n",
    "        else:\n",
    "            # calculate anomalies with provided climatology\n",
    "            DailyVars=(DailyVars - Normalize[2][None,:])/Normalize[2][None,:]\n",
    "\n",
    "    if len(DailyVars.shape) == 3:\n",
    "        DailyVars = DailyVars[:,:,:,None]\n",
    "    # Spatially smooth the data\n",
    "    DailyVars=gaussian_filter(DailyVars[:,:,:,:], sigma=(0,SmoothSigma,SmoothSigma,0))\n",
    "\n",
    "    # Remove the annual cycle\n",
    "    if RemoveAnnualCycl == 1:\n",
    "        SpatialMeanData=pd.DataFrame(np.nanmean(DailyVars, axis=(1,2)))\n",
    "        Averaged=np.roll(np.array(SpatialMeanData.rolling(window=21).mean()), -10, axis=0)\n",
    "        Averaged[:10,:]=Averaged[11,:][None,:]; Averaged[-10:,:]=Averaged[-11,:][None,:]\n",
    "        DailyVars=DailyVars-Averaged[:,None,None,:]\n",
    "\n",
    "    # Normalize the data\n",
    "    if NormalizeData == 'D':\n",
    "        DailyVars=(DailyVars-np.mean(DailyVars, axis=(1,2))[:,None,None,:])/np.std(DailyVars, axis=(1,2))[:,None,None,:]\n",
    "    elif NormalizeData == 'C':\n",
    "        if Normalize is None:\n",
    "            if ReferencePer is None:\n",
    "                DailyVars=(DailyVars-np.mean(DailyVars, axis=(0,1,2))[None,None,None,:])/np.std(DailyVars, axis=(0,1,2))[None,None,None,:]\n",
    "            else:\n",
    "                DailyVars=(DailyVars-np.mean(DailyVars[ReferencePer], axis=(0,1,2))[None,None,None,:])/np.std(DailyVars[ReferencePer], axis=(0,1,2))[None,None,None,:]\n",
    "        else:\n",
    "            # use predefined normalization terms\n",
    "            DailyVars=((DailyVars - Normalize[0][None,None,None,:]))/Normalize[1][None,None,None,:]\n",
    "        DailyVars[np.isnan(DailyVars)]=0\n",
    "\n",
    "    return DailyVars\n",
    "\n",
    "# ===================================================================\n",
    "def EucledianDistance(DailyVars,\n",
    "                      rgrClustersFin,\n",
    "                      MoreDistances=0):  # if this key is 1 the function will calculate additional distance metrics\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    SHAPE=DailyVars.shape\n",
    "    Data_flatten=np.reshape(DailyVars, (SHAPE[0],SHAPE[1]*SHAPE[2]*SHAPE[3]))\n",
    "    EucledianDist=np.zeros((SHAPE[0],rgrClustersFin[0].shape[0])); EucledianDist[:]=np.nan\n",
    "    Correlation=np.copy(EucledianDist)\n",
    "    Manhattan=np.copy(EucledianDist)\n",
    "    Chebyshev=np.copy(EucledianDist)\n",
    "    for dd in range(SHAPE[0]):\n",
    "        EucledianDist[dd,:] = np.array([np.linalg.norm(rgrClustersFin[0][wt,:]-Data_flatten[dd,:]) for wt in range(rgrClustersFin[0].shape[0])])\n",
    "        Correlation[dd,:] = np.array([np.corrcoef(rgrClustersFin[0][wt,:],Data_flatten[dd,:])[0][1] for wt in range(rgrClustersFin[0].shape[0])])\n",
    "        \n",
    "        if MoreDistances == 1:\n",
    "            for wt in range(rgrClustersFin[0].shape[0]):\n",
    "                x = Data_flatten[dd,:] #rgrClustersFin[0][wt,:]\n",
    "                YY = rgrClustersFin[0][wt,:] #+np.random.rand(len(rgrClustersFin[0][wt,:]))\n",
    "                XX = Data_flatten[dd,:]\n",
    "\n",
    "                # ----- Manhattan Distance ------\n",
    "                # Quoting from the paper, “On the Surprising Behavior of Distance Metrics in High Dimensional Space”, by Charu C. Aggarwal, Alexander \n",
    "                # Hinneburg, and Daniel A. Kiem. “ for a given problem with a fixed (high) value of the dimensionality d, it may be preferable to use \n",
    "                # lower values of p. This means that the L1 distance metric (Manhattan Distance metric) is the most preferable for high dimensional applications.”\n",
    "                Manhattan[dd,wt] = distance.cityblock(XX, YY)\n",
    "                Chebyshev[dd,wt] = distance.chebyshev(XX, YY)\n",
    "    \n",
    "    return EucledianDist, Correlation, Manhattan, Chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed88b8d3-b45b-447f-b33b-bd5da7a1bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grNormalization(data_4d, vtx, wts, n_lat, n_lon):\n",
    "    grNormalizationFactors = {}\n",
    "    Mean = np.nanmean(data_4d, axis=(0,1))\n",
    "\n",
    "    # interpolate to centroid grid   \n",
    "    Mean_wt = interpolate(Mean.flatten(), vtx, wts).reshape(n_lat, n_lon) \n",
    "\n",
    "    # reshape Mean by adding back axis=(1,2) because of np.nanmean\n",
    "    Anomalies = (data_4d - Mean[None,None,:,:])/Mean[None,None,:,:]\n",
    "\n",
    "    # Convert input scalar to 1d array with at least one dimension\n",
    "    # in order to call function  PreprocessWTdata\n",
    "    Normalize = [np.atleast_1d(np.nanmean(Anomalies)), np.atleast_1d(np.nanstd(Anomalies)), Mean_wt[:,:,None]]\n",
    "    grNormalizationFactors[0] = Normalize\n",
    "    print('Amomalies', np.nanmean(Anomalies))\n",
    "    print(flnm_n)\n",
    "    np.savez(flnm_n, grNormalizationFactors = grNormalizationFactors)\n",
    "    #NormData = np.load(flnm_n, allow_pickle=True)\n",
    "    # NormData = NormData['grNormalizationFactors']    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5cc36-e33f-4072-895d-55cc4ec109e9",
   "metadata": {},
   "source": [
    "#### User setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05abfc8d-469a-48ad-ab2d-02ad5f6479dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegName = 'Arizona West' # can be 'Arizona West', 'Arizona East', 'NM North', and 'NM South'\n",
    "#RegName = 'Arizona East'\n",
    "#RegName = 'NM North'\n",
    "#RegName = 'NM South'\n",
    "MONTHS  = [6,7,8,9,10]    # months within the NAM season\n",
    "month_day_dict = {6: 30, 7:31, 8:31, 9:30, 10:31}\n",
    "\n",
    "month_day_dict[10] # [MONTHS[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e6071-d160-4467-a067-9180622f137b",
   "metadata": {},
   "source": [
    "#### Load preprocessed data for assigning weather types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57afca86-06bb-40a8-b1db-13003bdbadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup clustering algorithm\n",
    "ClusterMeth='HandK'  # current options are ['HandK','hdbscan']\n",
    "ClusterBreakup = 0   # breakes up clusters that are unproportionally large (only for hdbscan)\n",
    "RelAnnom=1           # 1 - calculates daily relative anomalies\n",
    "NormalizeData='C'    # normalize variables | options are  - 'C' - climatology\n",
    "                                                        # - 'D' - daily (default)\n",
    "                                                        # - 'N' - none\n",
    "MinDistDD=1          # minimum nr of days between XWT events\n",
    "RemoveAnnualCycl=0   # remove annual cycle in varaiables with 21 day moving average filter\n",
    "\n",
    "dir_data = '/glade/campaign/mmm/c3we/mingge/COEXIST/AZ_S2S_DATA/'\n",
    "if RegName == 'Arizona West':\n",
    "    # cluster 1501\n",
    "    sCentroidFile = dir_data + '1501_XWT-centroids_train-1982-2018_eval-1982-2018_E13514_XWTs3_Vars-Q850_M-6-7-8-9-10.nc'\n",
    "    sERA_data = dir_data + 'ERA-Interim_PRISM_data13514_1501_1982-2018_Q850_JJASO.npz'\n",
    "    sWTdata = dir_data + 'Clusters13514_1501_1982-2018_Q850_JJASO'\n",
    "    NormFact = dir_data + 'Normalization_Factors_IFS_1501.npz'\n",
    "    WTsort = [0,2,1]\n",
    "    re = 0\n",
    "if RegName == 'Arizona East':\n",
    "    # cluster 1502\n",
    "    sERA_data = dir_data + 'ERA-Interim_PRISM_data13514_1502_1982-2018_Q850_JJASO.npz'\n",
    "    sCentroidFile = dir_data +'1502_XWT-centroids_train-1982-2018_eval-1982-2018_E13514_XWTs3_Vars-Q850_M-6-7-8-9-10.nc'\n",
    "    sWTdata = dir_data + 'Clusters13514_1502_1982-2018_Q850_JJASO'\n",
    "    NormFact = dir_data + 'Normalization_Factors_IFS_1502.npz'\n",
    "    WTsort = [1,2,0]\n",
    "    re = 1\n",
    "if RegName == 'NM North':\n",
    "    # cluster HUC6-00\n",
    "    sERA_data = dir_data + 'ERA-Interim_PRISM_data13514_HUC6-00_1982-2018_Q850_JJASO.npz'\n",
    "    sCentroidFile = dir_data + 'HUC6-00_XWT-centroids_train-1982-2018_eval-1982-2018_E13514_XWTs3_Vars-Q850_M-6-7-8-9-10.nc'\n",
    "    sWTdata = dir_data + 'Clusters13514_HUC6-00_1982-2018_Q850_JJASO'\n",
    "    NormFact = dir_data + 'Normalization_Factors_IFS_HUC6-00.npz'\n",
    "    WTsort = [1,2,0]\n",
    "    re = 2\n",
    "if RegName == 'NM South':\n",
    "    # cluster HUC6-03\n",
    "    sERA_data = dir_data + 'ERA-Interim_PRISM_data13514_HUC6-03_1982-2018_Q850_JJASO.np'\n",
    "    sCentroidFile = dir_data + 'HUC6-03_XWT-centroids_train-1982-2018_eval-1982-2018_E13514_XWTs3_Vars-Q850_M-6-7-8-9-10.nc'\n",
    "    sWTdata = dir_data + 'Clusters13514_HUC6-03_1982-2018_Q850_JJASO'\n",
    "    NormFact = dir_data + 'Normalization_Factors_IFS_HUC6-03.npz'\n",
    "    WTsort = [0,1,2]\n",
    "    re = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50544eac-0a10-46fb-bbcc-bcd697723d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids: /glade/campaign/mmm/c3we/mingge/COEXIST/AZ_S2S_DATA/Clusters13514_1501_1982-2018_Q850_JJASO\n",
      "centroid shape: (18, 17)\n"
     ]
    }
   ],
   "source": [
    "# Load the Centroids\n",
    "print('Centroids:', sWTdata)\n",
    "with open(sWTdata, 'rb') as handle:\n",
    "    npzfile = pickle.load(handle)\n",
    "WTclusters=npzfile['grClustersFin']['Full']\n",
    "WTlat=npzfile['LatWT']#; rgrLatT1 = WTlat\n",
    "WTlon=npzfile['LonWT']#; rgrLonT1 = WTlon\n",
    "# WTlon[WTlon<0] = WTlon[WTlon<0]+360\n",
    "WTtime=npzfile['rgdTime']\n",
    "SpatialSmoothing=npzfile['SpatialSmoothing']\n",
    "print('centroid shape:', WTlat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f04fbaf-488d-4d77-8fe6-6e434c1b7cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/fs1/collections/rda/data/ds627.0/ei.oper.an.pl/200112/ei.oper.an.pl.regn128sc.2001121812\n",
      "<xarray.Dataset>\n",
      "Dimensions:        (isobaricInhPa: 37, latitude: 256, longitude: 512)\n",
      "Coordinates:\n",
      "    number         int64 ...\n",
      "    time           datetime64[ns] ...\n",
      "    step           timedelta64[ns] ...\n",
      "  * isobaricInhPa  (isobaricInhPa) int64 1000 975 950 925 900 875 ... 7 5 3 2 1\n",
      "  * latitude       (latitude) float64 89.46 88.77 88.07 ... -88.07 -88.77 -89.46\n",
      "  * longitude      (longitude) float64 0.0 0.7031 1.406 ... 357.9 358.6 359.3\n",
      "    valid_time     datetime64[ns] ...\n",
      "Data variables:\n",
      "    pv             (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    z              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    t              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    q              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    w              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    vo             (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    d              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    r              (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    o3             (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    clwc           (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    ciwc           (isobaricInhPa, latitude, longitude) float32 ...\n",
      "    cc             (isobaricInhPa, latitude, longitude) float32 ...\n",
      "Attributes:\n",
      "    GRIB_edition:            1\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2023-07-18T14:36:32 GRIB to CDM+CF via cfgrib-0....\n",
      "ERA-INTERIM shape: (27, 25)\n"
     ]
    }
   ],
   "source": [
    "# read ERA-INTERIM coordinates\n",
    "dir_erai = '/gpfs/fs1/collections/rda/data/ds627.0/ei.oper.an.pl/'\n",
    "flnm = dir_erai + '200112/ei.oper.an.pl.regn128sc.2001121812'\n",
    "print(flnm)\n",
    "\n",
    "ds = xr.open_dataset(flnm, engine=\"cfgrib\", backend_kwargs={'indexpath' :'' })\n",
    "print(ds)\n",
    "\n",
    "rgrLonS=ds['longitude'][:]\n",
    "rgrLatS=ds['latitude'][:]\n",
    "n_lat_i = rgrLatS.shape[0]\n",
    "n_lon_i = rgrLonS.shape[0] \n",
    " \n",
    "rgrLonSF, rgrLatSF = np.meshgrid(rgrLonS, rgrLatS)\n",
    "rgrLonSF[rgrLonSF>180] = rgrLonSF[rgrLonSF>180]-360\n",
    "\n",
    "# !!!!  NOTE: reverse Lat: make it from south to North\n",
    "rgrLatSF = rgrLatSF[::-1] # ERA-Interim lat runs from N to S\n",
    " \n",
    "# get the region of interest\n",
    "iAddCells= 4 # grid cells added to subregion\n",
    "iWest=np.argmin(np.abs(WTlon.min() - rgrLonSF[0,:]))-iAddCells\n",
    "iEast=np.argmin(np.abs(WTlon.max() - rgrLonSF[0,:]))+iAddCells\n",
    "iNort=np.argmin(np.abs(WTlat.max() - rgrLatSF[:,0]))+iAddCells\n",
    "iSouth=np.argmin(np.abs(WTlat.min() - rgrLatSF[:,0]))-iAddCells\n",
    "\n",
    "rgrLonS=rgrLonSF[iSouth:iNort,iWest:iEast]\n",
    "rgrLatS=rgrLatSF[iSouth:iNort,iWest:iEast]\n",
    "\n",
    "print('ERA-INTERIM shape:',rgrLonS.shape)\n",
    "\n",
    "# create gregridding weights\n",
    "# Remap ECMWF to ERA-Interim\n",
    "# WTlon.flatten() -> WTlon.flatten()[:,None]\n",
    "#          (306,) -> (306, 1)\n",
    "# wts.shape (306, 3)\n",
    "points=np.array([rgrLonS.flatten(), rgrLatS.flatten()]).transpose()\n",
    "vtx, wts = interp_weights(points, np.append(WTlon.flatten()[:,None], WTlat.flatten()[:,None], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cba7b-ccc0-4a5a-9026-f370cc7692b7",
   "metadata": {},
   "source": [
    "#### Calculate Normalization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f3fd2e5-16f5-48de-8869-fc39d8422bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 Days in [6, 7, 8, 9, 10]\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/AZ_S2S_DATA/NormalizationFactors_erai_Arizona West.npz exist\n"
     ]
    }
   ],
   "source": [
    "year_s = 1979 \n",
    "year_e = 2018\n",
    "#year_e = year_s\n",
    "n_year = year_e - year_s + 1\n",
    "\n",
    "# there are n_day_s2s = 153 days from June to Oct\n",
    "dStartDay=datetime(year_s, int(MONTHS[0]), 1,  12)\n",
    "dStopDay=datetime(year_s, int(MONTHS[-1]), 31, 12)\n",
    "rgdTimeDD = pd.date_range(dStartDay, end=dStopDay, freq='d')\n",
    "n_day_s2s = len(rgdTimeDD)\n",
    "print(n_day_s2s, 'Days in', MONTHS)\n",
    "\n",
    "dir_o = '/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/'\n",
    "dir_q = '/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/' + RegName.replace(\" \", \"_\")+ '/'\n",
    "\n",
    "if os.path.isdir(dir_q) != 1:\n",
    "    subprocess.call([\"mkdir\",\"-p\", dir_q])\n",
    "    print('mkdir ', dir_q)\n",
    "        \n",
    "data_4d = np.zeros((n_year, n_day_s2s, rgrLonS.shape[0], rgrLonS.shape[1]))\n",
    "\n",
    "flnm_n = dir_data + 'NormalizationFactors_erai_' + RegName + '.npz'\n",
    "\n",
    "if os.path.isfile(flnm_n):\n",
    "    print(flnm_n, 'exist')\n",
    "    ds_n = np.load(flnm_n, allow_pickle=True)\n",
    "    Normalize = ds_n['grNormalizationFactors'] \n",
    "else:\n",
    "    for year in range(year_s, year_e+1): \n",
    "        is_first = True\n",
    "        flnm_o = dir_q + 'q850_' + str(year) + '.nc'\n",
    "        if os.path.isfile(flnm_o):\n",
    "            print(flnm_o, 'exist')\n",
    "            with  xr.open_dataset(flnm_o) as ds:\n",
    "                 data_4d[year-year_s] = ds.q[:,::-1,:]   \n",
    "        else:\n",
    "            for month in MONTHS:\n",
    "                yyyymm_s = str(year*100 + month)\n",
    "                print(yyyymm_s)\n",
    "                \n",
    "                for day in range(1,month_day_dict[month]+1):\n",
    "                    for hr_s in ['00','06','12','18']:\n",
    "                        flnm = dir_erai + yyyymm_s + \"/ei.oper.an.pl.regn128sc.\" + yyyymm_s + str(day).zfill(2)+hr_s\n",
    "                        #print(flnm)\n",
    "                 \n",
    "                        ds = xr.open_dataset(flnm, engine=\"cfgrib\", backend_kwargs={'indexpath' :'' })\n",
    "                        # select 850hPa, crop to Centroid domain, daily mean\n",
    "                        # Follow original ERA from North to South\n",
    "                        q850_2d_0 = ds.q.sel(isobaricInhPa=850).isel(latitude=slice(n_lat_i - iNort, n_lat_i-iSouth)\n",
    "                                                      ,longitude=slice(iWest, iEast))\n",
    "                        if hr_s == '00':\n",
    "                            q850_2d = q850_2d_0\n",
    "                        else:\n",
    "                            q850_2d = q850_2d + q850_2d_0\n",
    "                    q850_2d = q850_2d/4    \n",
    "                    \n",
    "                    # concatenate daily data \n",
    "                    if is_first:\n",
    "                        q850_3d = q850_2d\n",
    "                        is_first = False\n",
    "                    else:\n",
    "                        q850_3d = xr.concat([q850_3d, q850_2d], dim=\"time\") \n",
    "                            \n",
    "            print(flnm_o)\n",
    "            q850_3d.to_netcdf(path = flnm_o)  \n",
    "            data_4d[year-year_s] =q850_3d[:,::-1,:]\n",
    "    \n",
    "    # data_4d from South to North\n",
    "    Mean = np.nanmean(data_4d, axis=(0,1))\n",
    "    # interpolate to centroid grid   \n",
    "    Mean_wt = interpolate(Mean.flatten(), vtx, wts).reshape(WTlon.shape[0],WTlat.shape[1]) \n",
    "\n",
    "    # reshape Mean by adding back axis=(1,2) because of np.nanmean\n",
    "    Anomalies = (data_4d - Mean[None,None,:,:])/Mean[None,None,:,:]\n",
    "\n",
    "    # Convert input scalar to 1d array with at least one dimension\n",
    "    # in order to call function  PreprocessWTdata\n",
    "    Normalize = [np.atleast_1d(np.nanmean(Anomalies)), np.atleast_1d(np.nanstd(Anomalies)), Mean_wt]\n",
    "     \n",
    "    print('Amomalies', np.nanmean(Anomalies))\n",
    "    print(flnm_n)\n",
    "    np.savez(flnm_n, grNormalizationFactors = Normalize)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3639efd0-b7f7-458e-97b7-43411218efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d0650-ac5a-41d3-b6c5-fc3769599172",
   "metadata": {},
   "source": [
    "#### Read ERA-INTERIM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb782e97-e3dd-407d-9c08-740edd889c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1979.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1979.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1980.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1980.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1981.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1981.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1982.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1982.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1983.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1983.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1984.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1984.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1985.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1985.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1986.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1986.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1987.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1987.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1988.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1988.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1989.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1989.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1990.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1990.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1991.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1991.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1992.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1992.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1993.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1993.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1994.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1994.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1995.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1995.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1996.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1996.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1997.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1997.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1998.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1998.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_1999.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-1999.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2000.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2000.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2001.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2001.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2002.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2002.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2003.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2003.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2004.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2004.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2005.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2005.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2006.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2006.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2007.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2007.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2008.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2008.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2009.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2009.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2010.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2010.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2011.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2011.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2012.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2012.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2013.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2013.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2014.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2014.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2015.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2015.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2016.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2016.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2017.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2017.csv\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/Q850/Arizona_West/q850_2018.nc\n",
      "/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/ERAI_WT-frequency_Arizona-West-2018.csv\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "dir_i = dir_q\n",
    "dir_o = '/glade/campaign/mmm/c3we/mingge/COEXIST/ERAI/WT-frequency/'\n",
    "\n",
    "if os.path.isdir(dir_o) != 1:\n",
    "    subprocess.call([\"mkdir\", \"-p\", dir_o])\n",
    "    print('mkdir ', dir_o)\n",
    "        \n",
    "for year in range(year_s, year_e + 1):\n",
    "    flnm_i = dir_q + 'q850_' + str(year) + '.nc'\n",
    "    print(flnm_i)\n",
    "    with  xr.open_dataset(flnm_i) as ds:\n",
    "        DataAct = ds.q[:,::-1,:].values  \n",
    "                    \n",
    "        Data_on_centroid_grid = np.zeros((n_day_s2s, WTlon.shape[0], WTlat.shape[1]))\n",
    "        Data_on_centroid_grid[:] = np.nan\n",
    "\n",
    "        for tt in range(n_day_s2s):\n",
    "            valuesi=interpolate(DataAct[tt,:,:].flatten(), vtx, wts)\n",
    "            Data_on_centroid_grid[tt,:,:] = valuesi.reshape(WTlon.shape[0],WTlat.shape[1])\n",
    "\n",
    "        # Associiate dta to WTs\n",
    "        SHAPE = Data_on_centroid_grid.shape\n",
    "        WT_NMME = np.zeros((SHAPE[0])) \n",
    "        WT_NMME[:] = np.nan\n",
    "   \n",
    "        if np.isnan(np.nanmean(Data_on_centroid_grid[:,:,:])) == False:\n",
    "            isNAN = np.isnan(np.nanmean(Data_on_centroid_grid[:,:,:], axis=(1,2)))\n",
    "            DailyVarsEvalNorm=PreprocessWTdata(Data_on_centroid_grid,               # WT data [time,lat,lon,var]\n",
    "                            RelAnnom=RelAnnom,                                              # calculate relative anomalies [1-yes; 0-no]\n",
    "                            SmoothSigma=0,                                                  # Smoothing stddev (Gaussian smoothing)\n",
    "                            RemoveAnnualCycl=RemoveAnnualCycl,                              # remove annual cycle [1-yes; 0-no]\n",
    "                            NormalizeData=NormalizeData,                                    # normalize data [1-yes; 0-no]\n",
    "                            Normalize = Normalize)                                           # predefined mean and std for normalization  \n",
    "\n",
    "            EucledianDist, Correlation, Manhattan, Chebyshev = EucledianDistance(DailyVarsEvalNorm,\n",
    "                                                      WTclusters)\n",
    "            EucledianDist_orig=np.copy(EucledianDist)\n",
    "            EucledianDist=np.nanmin(EucledianDist,axis=1)\n",
    "            MinED = np.nanargmin(EucledianDist_orig,axis=1).astype(float)\n",
    "            MinED[isNAN] = np.nan\n",
    "            WT_NMME = MinED\n",
    "\n",
    "        # Re-label the WTs so that: 0=Monsoon, 1=Normal, 2=Dry\n",
    "        WT_NMME_FIN = np.copy(WT_NMME); WT_NMME_FIN[:] = 999\n",
    "        for wt in range(len(WTsort)):\n",
    "            WT_NMME_FIN[WT_NMME == WTsort[wt]] = int(wt)\n",
    "            \n",
    "        ### Process the output into monthly mean WT frequencies and save them into a csv file\n",
    "        WT_MM = np.zeros((5, 3))\n",
    "        rgiDDmon = [30,31,31,30,31]\n",
    "        for wt in range(3):\n",
    "            for mm in range(5):\n",
    "                WT_MM[mm,wt] = np.sum(WT_NMME_FIN[int(np.sum(rgiDDmon[:mm])):int(np.sum(rgiDDmon[:mm+1]))] == wt, axis=0)    \n",
    "        \n",
    "        YYYYMM = [ (year*100+mm) for mm in MONTHS]\n",
    "        DATA = np.round(WT_MM,2)\n",
    "        \n",
    "        grDATA = {}\n",
    "        grDATA['YYYYMM'] = YYYYMM\n",
    "        grDATA['dry']  = DATA[:len(YYYYMM),2].astype(float)\n",
    "        grDATA['normal']  = DATA[:len(YYYYMM),1].astype(float)\n",
    "        grDATA['monsoon']  = DATA[:len(YYYYMM),0].astype(float)\n",
    "        df = pd.DataFrame(grDATA)\n",
    "         \n",
    "        flnm_o = dir_o + 'ERAI_WT-frequency_'+ RegName.replace(' ','-') + '-'+ str(year) +'.csv'\n",
    "        print(flnm_o)\n",
    "        df.to_csv(flnm_o, index=False)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040388cf-2803-4cb8-82b9-4188ac139ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383073ab-660c-4a91-bf37-4f042c16657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-casper_2020]",
   "language": "python",
   "name": "conda-env-miniconda3-casper_2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
